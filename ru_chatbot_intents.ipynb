{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import converters\n",
    "import uuid\n",
    "import analyzers\n",
    "from ibm_watson import AssistantV2\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from sdk import Client, QnaAPI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just AI\n",
    "## - Select classifier type in Just AI UI - STS, Deep Learning or other.\n",
    "## https://app.jaicp.com\n",
    "\n",
    "# Just AI prepare Train\n",
    "train_path = 'data/russian/chatbot-intents/train-chatbot.csv'\n",
    "docs, X, y = converters.parse_data_csv(train_path)\n",
    "\n",
    "data_dict = {'Group': [], 'Question': [], 'Enabled': [], 'Alternative phrases-Text': [], 'Alternative phrases-Patterns': [], 'Answer': []}\n",
    "for doc in docs.values():\n",
    "    data_dict['Question'].append(doc['question'])\n",
    "    qs = \"\\n\".join([q.replace(\"\\n\", \" \") for q in doc['paraphrased_questions']])    \n",
    "    data_dict['Alternative phrases-Text'].append(qs)\n",
    "    data_dict['Enabled'].append('true')\n",
    "    data_dict['Answer'].append(doc['answer'])\n",
    "    data_dict['Group'].append(None)\n",
    "    data_dict['Alternative phrases-Patterns'].append(None)\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.to_excel('data/russian/hwu-20-ru/train-justai.xlsx', index=False)\n",
    "\n",
    "# Goto Just AI UI and upload this excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just AI testing. Copy token from UI\n",
    "token = 'xxx'\n",
    "_, X_test, y_test = converters.parse_data_csv('data/russian/chatbot-intents/test-chatbot.csv')\n",
    "times = []\n",
    "y_pred = []\n",
    "for i, row in enumerate(X_test):\n",
    "    print(i)\n",
    "    start = time.time()\n",
    "    r = requests.get(f'https://app.jaicp.com/cailapub/api/caila/p/{token}/nlu/inference', params={'query': row})\n",
    "    times.append(time.time() - start)\n",
    "    y_pred.append(r.json()['intent']['answer'])\n",
    "\n",
    "new_y_pred = []\n",
    "for p in y_pred:\n",
    "    if p is None:\n",
    "        new_y_pred.append(\"other\")\n",
    "    else:\n",
    "        new_y_pred.append(p)\n",
    "\n",
    "print(classification_report(y_test, new_y_pred))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, new_y_pred)) \n",
    "print(\"F1-Score: \", f1_score(y_test, new_y_pred, average='macro')) \n",
    "\n",
    "print(f\"Mean response time: {np.mean(times)} +- {np.std(times)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autofaq http://chat.autofaq.ai\n",
    "# Contact info@autofaq.ai to get user_id and user_token\n",
    "user_id = 11\n",
    "user_token = 'xxx'\n",
    "service_response = {}\n",
    "# service_response = {'service_id': 12345, 'tokens': ['xxx']}\n",
    "publish_time = 0\n",
    "\n",
    "host_url = 'https://chat.autofaq.ai'\n",
    "namespace = 'core-api/crud/api/v1'\n",
    "# api_url = 'https://chat.autofaq.ai/core-api/query'\n",
    "api_url = 'https://api.autofaq.ai/v1' \n",
    "\n",
    "test_path = 'data/russian/chatbot-intents/test-chatbot.csv'\n",
    "train_path = 'data/russian/chatbot-intents/train-chatbot.csv'\n",
    "\n",
    "def metrics_calc(test_path, train_path, user_id, user_token, name, namespace, service_response):\n",
    "    docs_test, X_test, y_test = converters.parse_data_csv(test_path)\n",
    "    if not service_response:\n",
    "        docs_train, _, _ = converters.parse_data_csv(train_path)\n",
    "        print(\"Parsed data\")\n",
    "        \n",
    "        client = Client(host_url=host_url, user_id=user_id, user_token=user_token, namespace=namespace)\n",
    "        Client.HTTP_TIMEOUT = 180\n",
    "\n",
    "        service_response = client.create_service({'preset': 'ru', 'name': name, 'skip_publish': True})\n",
    "        for doc in docs_train.values():\n",
    "            client.create_document(\n",
    "                service_response['service_id'], \n",
    "                question=doc['question'], \n",
    "                answer=doc['answer'], \n",
    "                name=doc['name'], \n",
    "                paraphrases=doc['paraphrased_questions']\n",
    "            )\n",
    "        publish_time = time.time()\n",
    "        client.publish_service(service_response['service_id'], wait_timeout=600)\n",
    "        publish_time = time.time() - publish_time\n",
    "        print(\"Service published for {} seconds\".format(publish_time))\n",
    "\n",
    "    qna = QnaAPI(api_url, service_response['service_id'], service_response['tokens'][0])\n",
    "\n",
    "    print(\"Querying API ...\")\n",
    "    test_results = []\n",
    "    times = []\n",
    "    for row in X_test:\n",
    "        start_time = time.time()\n",
    "        res = qna.query(row)\n",
    "        times.append(time.time() - start_time)\n",
    "        test_results.append(res)\n",
    "\n",
    "    y_pred = []\n",
    "    for r in test_results:\n",
    "        y_pred.append(r['results'][0]['name'])\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred)) \n",
    "    print(\"F1-Score: \", f1_score(y_test, y_pred, average='macro'))   \n",
    "    print(f\"Mean response time: {np.mean(times)} +- {np.std(times)} sec.\")\n",
    "    print(f\"Publish time: {publish_time} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autofaq metrics calculation\n",
    "metrics_calc(test_path, train_path, user_id, user_token, 'Hwu-20-ru', namespace, service_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Autofaq Pro Experimental\n",
    "pd.DataFrame({'is_test': ['test']*len(X_test), 'question': X_test, 'label': y_test}).to_csv('data/russian/hwu-20-ru/test-af-pro-experimental.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cognigy http://cognigy.com/\n",
    "train_path = 'data/russian/chatbot-intents/train-chatbot.csv'\n",
    "test_path = 'data/russian/chatbot-intents/test-chatbot.csv'\n",
    "docs, X, y = converters.parse_data_csv(train_path)\n",
    "mock = ['exampleSentence']*len(X)\n",
    "pd.DataFrame(list(zip(y, mock, X)), columns=None, index=None).to_csv('data/russian/hwu-20-ru/train-cognigy.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Go to https://trial.cognigy.ai/login\n",
    "# 2. Create Flow with Intents\n",
    "# 3. Import Intents using CSV import in Web interface\n",
    "# 4. Build model in Web Interface\n",
    "# 5. Add Code node to Flow Chart with following code (actions.output(\"intentScore:\" + input.intentScore +\";intent:\"+input.intent);)\n",
    "# 5. Query from here\n",
    "\n",
    "# Get Cognigy api key from their website\n",
    "api_key = 'xxx'\n",
    "def query_api(text):\n",
    "  # Get Cognigy rest_endpoint from their website\n",
    "  rest_endpoint = 'https://endpoint-trial.cognigy.ai/xxx'\n",
    "  rest_params = {\n",
    "    \"userId\":\"312312\",\n",
    "    \"sessionId\": uuid.uuid4().hex,\n",
    "    \"text\": text,\n",
    "    \"api_key\": api_key,\n",
    "  }\n",
    "  response = requests.post(rest_endpoint, json=rest_params)\n",
    "  return response\n",
    "\n",
    "def get_intent_name(response):\n",
    "    splitted = response.json()['text'].split(';')\n",
    "    intent_score = splitted[0]\n",
    "    intent_name_raw = splitted[1]\n",
    "    intent_name = intent_name_raw.split(\"intent:\")[1].replace(\" __GARBAGE__\", \"\")\n",
    "    return intent_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cognigy testing\n",
    "times = []\n",
    "_, X, y = converters.parse_data_csv(test_path)\n",
    "y_pred = []\n",
    "for i, sent in enumerate(X):\n",
    "    if i > 75:\n",
    "        start_time = time.time()\n",
    "        response = query_api(sent)\n",
    "        times.append(time.time() - start_time)\n",
    "        y_pred.append(get_intent_name(response))\n",
    "        \n",
    "print(classification_report(y, y_pred))\n",
    "print(\"Accuracy: \", accuracy_score(y, y_pred)) \n",
    "print(\"F1-Score: \", f1_score(y, y_pred, average='macro')) \n",
    "\n",
    "print(f\"Mean response time: {np.mean(times)} +- {np.std(times)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Vertex https://console.cloud.google.com/vertex-ai/\n",
    "docs, X, y = converters.parse_data_csv(train_path)\n",
    "X_train, X_devtest, y_train, y_devtest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_devtest, y_devtest, test_size=0.5, random_state=42)\n",
    "# dataframe for google vertex auto ml\n",
    "df = pd.DataFrame(columns=['type', 'text', 'label'])\n",
    "df['type'] = ['test']*len(X_test) + ['training']*len(X_train) + ['validation']*len(X_dev)\n",
    "df['text'] = X_test + X_train + X_dev\n",
    "df['label'] = y_test + y_train + y_dev\n",
    "df.to_csv('data/russian/hwu-20-ru/google_vertex.csv', index=False, header=False)\n",
    "\n",
    "# You can upload this csv to google vertex UI. But model cant be built because of too small training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ibm watson https://cloud.ibm.com/catalog/services/watson-assistant\n",
    "docs, X, y = converters.parse_data_csv(train_path)\n",
    "pd.DataFrame(list(zip(X, y)), columns=None, index=None).to_csv('data/russian/hwu-20-ru/train-watson-X-y.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get assistant_id, apikey, url from watson ui\n",
    "assistant_id = \"xxx\"\n",
    "apikey = \"xxx\"\n",
    "url = \"https://api.eu-de.assistant.watson.cloud.ibm.com\"\n",
    "authenticator = IAMAuthenticator(f'{apikey}')\n",
    "assistant = AssistantV2(\n",
    "    version='2021-06-14',\n",
    "    authenticator = authenticator\n",
    ")\n",
    "\n",
    "assistant.set_service_url(f'{url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watson predict\n",
    "docs, X, y = converters.parse_data_csv(test_path)\n",
    "y_preds = []\n",
    "times = []\n",
    "for sent in X:\n",
    "    query_time = time.time()\n",
    "    response = assistant.message_stateless(\n",
    "        assistant_id=f'{assistant_id}',\n",
    "        input={\n",
    "            'message_type': 'text',\n",
    "            'text': sent\n",
    "        }).get_result()  \n",
    "    try:  \n",
    "        y_preds.append(response['output']['intents'][0]['intent'])\n",
    "    except IndexError:\n",
    "        print(response)\n",
    "        y_preds.append('other')\n",
    "    query_time = time.time() - query_time\n",
    "    times.append(query_time)\n",
    "print(classification_report(y, y_preds))\n",
    "print(\"Accuracy: \", accuracy_score(y, y_preds)) \n",
    "print(\"F1-Score: \", f1_score(y, y_preds, average='macro')) \n",
    "print(f\"Mean response time: {np.mean(times)} +- {np.std(times)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogflow http://dialogflow.cloud.google.com\n",
    "dc = converters.DialogflowConverter()\n",
    "dc.import_corpus(train_path, 'HWU_RuBench_DialogFlow', 'ru')\n",
    "dc.export(\"HWU_RuBench_DialogFlow.zip\")\n",
    "# Upload this zip to dialogflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your app_id\n",
    "app_id = ''\n",
    "# put your gcloud credentials in env_path below, e.g. /home/xxx/.config/gcloud/application_default_credentials.json\n",
    "env_path = ''\n",
    "da = analyzers.DialogflowAnalyser(app_id, env_path)\n",
    "docs, X, y = converters.parse_data_csv(test_path)\n",
    "dialogflow_results, times = da.get_annotations(X, 'ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogflow test\n",
    "y_preds = [r.query_result.intent.display_name for r in dialogflow_results]\n",
    "\n",
    "print(classification_report(y, y_preds))\n",
    "print(\"Accuracy: \", accuracy_score(y, y_preds)) \n",
    "print(\"F1-Score: \", f1_score(y, y_preds, average='macro')) \n",
    "\n",
    "print(f\"Mean response time: {np.mean(times)} +- {np.std(times)} sec.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21c7fd78f0d018c2c0a6a5e1aa58ee4fa1352db66784041491044956b8271de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
